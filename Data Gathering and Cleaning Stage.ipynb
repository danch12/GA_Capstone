{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T16:48:11.292454Z",
     "start_time": "2019-12-16T16:48:07.381418Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "#look at median income _std too\n",
    "#look at quantile boundries\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_score\n",
    "import xgboost as xgb\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part I am going to use a dataset that is provided by Lending club that has almost all the variables I need in a csv format- very handy. However there are a couple bits of information regarding the individual states such as average income that I will need to scrape off the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources of data\n",
    "loan data-https://www.lendingclub.com/info/download-data.action\n",
    "average household income-https://en.wikipedia.org/wiki/Household_income_in_the_United_States\n",
    "\n",
    "quartile income-\n",
    "https://data.census.gov/cedsci/table?q=median%20income&g=&hidePreview=true&table=S1901&tid=ACSST1Y2018.S1901&t=Income%20%28Households,%20Families,%20Individuals%29&lastDisplayedRow=16&vintage=2018&mode="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lending Club Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T16:51:29.675112Z",
     "start_time": "2019-12-16T16:50:42.449531Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (18,46,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('accepted_2007_to_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset is so large my computer could not handle the computations required to run models properly. Therefore to reduce the amount of data I cut rows with NA values in avg_cur_bal. I chose avg_cur_bal because rows with NA values in this column usually had a high amount of NA values in other columns. If I could only work with a limited amount of data it makes sense to give myself the best quality of data possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T16:52:27.492274Z",
     "start_time": "2019-12-16T16:52:27.311548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Current                                                783839\n",
       "Fully Paid                                             396204\n",
       "Charged Off                                             96554\n",
       "Late (31-120 days)                                      20073\n",
       "In Grace Period                                         10210\n",
       "Issued                                                   6048\n",
       "Late (16-30 days)                                        4555\n",
       "Does not meet the credit policy. Status:Fully Paid       1988\n",
       "Default                                                  1615\n",
       "Does not meet the credit policy. Status:Charged Off       761\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loan_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T16:52:38.319406Z",
     "start_time": "2019-12-16T16:52:36.290589Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df[np.isfinite(df['avg_cur_bal'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T16:53:19.054143Z",
     "start_time": "2019-12-16T16:53:17.480862Z"
    }
   },
   "outputs": [],
   "source": [
    "df['year_issued']=df.issue_d.map(lambda x : int(x.split('-')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Current loans are still going we should exclude them because we dont know if they are bad or good. going to consider fully paid as good. All the rest are bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T16:53:36.450543Z",
     "start_time": "2019-12-16T16:53:33.739796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fully Paid            339950\n",
       "Charged Off            86289\n",
       "Late (31-120 days)     20036\n",
       "In Grace Period        10180\n",
       "Issued                  6048\n",
       "Late (16-30 days)       4543\n",
       "Default                 1610\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[df.loan_status !='Current']\n",
    "df.loan_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T16:53:44.563731Z",
     "start_time": "2019-12-16T16:53:44.461362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fully Paid            0.725372\n",
       "Charged Off           0.184120\n",
       "Late (31-120 days)    0.042752\n",
       "In Grace Period       0.021722\n",
       "Issued                0.012905\n",
       "Late (16-30 days)     0.009694\n",
       "Default               0.003435\n",
       "Name: loan_status, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loan_status.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional supporting Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Average household Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I scrape additional data from Wikepedia and downloaded data from the American cencus. As lending club provides income information I thought it may be useful to see how the loanee's income compared to their state average, and further what income band the loanee would fall under. This may be more useful than a pure income number as for example 100K in Detroit may go a lot further than 100K in San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping data from tables on wikipedia\n",
    "URL='https://en.wikipedia.org/wiki/Household_income_in_the_United_States'\n",
    "\n",
    "r= requests.get(URL)\n",
    "soup=BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "state_list=[]\n",
    "income_2017=[]\n",
    "income_2016=[]\n",
    "income_2015=[]\n",
    "income_2014=[]\n",
    "income_2013=[]\n",
    "income_2012=[]\n",
    "income_2011=[]\n",
    "income_2010=[]\n",
    "income_2009=[]\n",
    "income_2008=[]\n",
    "income_2007=[]\n",
    "\n",
    "table= soup.find_all(attrs={'class':'wikitable'})\n",
    "income_table=table[5]\n",
    "for row in income_table.find_all('tr'):\n",
    "    for ind,col in enumerate(row.find_all('td')):\n",
    "        if ind==0 or ind==1:\n",
    "            pass\n",
    "        \n",
    "        if ind==2:\n",
    "            try:\n",
    "                state_list.append(col.text)\n",
    "            except:\n",
    "                state_list.append(np.Nan)\n",
    "                print('no')\n",
    "        if ind==3:\n",
    "            try:\n",
    "                income_2017.append(col.text)\n",
    "            except:\n",
    "                income_2017.append(np.Nan)\n",
    "                print('no')\n",
    "        if ind==4:\n",
    "            try:\n",
    "                income_2016.append(col.text)\n",
    "            except:\n",
    "                income_2016.append(np.Nan)\n",
    "                print('no')\n",
    "        if ind==5:\n",
    "            try:\n",
    "                income_2015.append(col.text)\n",
    "            except:\n",
    "                income_2015.append(np.Nan)\n",
    "                print('no')\n",
    "                \n",
    "        if ind==6:\n",
    "            try:\n",
    "                income_2014.append(col.text)\n",
    "            except:\n",
    "                income_2014.append(np.Nan)\n",
    "                print('no')\n",
    "        \n",
    "        if ind==7:\n",
    "            try:\n",
    "                income_2013.append(col.text)\n",
    "            except:\n",
    "                income_2013.append(np.Nan)\n",
    "                print('no')\n",
    "        \n",
    "        if ind==8:\n",
    "            try:\n",
    "                income_2012.append(col.text)\n",
    "            except:\n",
    "                income_2012.append(np.Nan)\n",
    "                print('no')\n",
    "                \n",
    "        if ind==9:\n",
    "            try:\n",
    "                income_2011.append(col.text)\n",
    "            except:\n",
    "                income_2011.append(np.Nan)\n",
    "                print('no')\n",
    "        \n",
    "        if ind==10:\n",
    "            try:\n",
    "                income_2010.append(col.text)\n",
    "            except:\n",
    "                income_2010.append(np.Nan)\n",
    "                print('no')\n",
    "                \n",
    "        if ind==11:\n",
    "            try:\n",
    "                income_2009.append(col.text)\n",
    "            except:\n",
    "                income_2009.append(np.Nan)\n",
    "                print('no')\n",
    "        \n",
    "        if ind==12:\n",
    "            try:\n",
    "                income_2008.append(col.text)\n",
    "            except:\n",
    "                income_2008.append(np.Nan)\n",
    "                print('no')\n",
    "                \n",
    "        if ind==13:\n",
    "            try:\n",
    "                income_2007.append(col.text)\n",
    "            except:\n",
    "                income_2007.append(np.Nan)\n",
    "                print('no')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe from the data scraped\n",
    "state_info=pd.DataFrame({'state':state_list,\n",
    "                            '2017': income_2017,\n",
    "                           '2016': income_2016,\n",
    "                           '2015':income_2015,\n",
    "                           '2014':income_2014,\n",
    "                           '2013':income_2013,\n",
    "                           '2012':income_2012,\n",
    "                           '2011':income_2011,\n",
    "                           '2010':income_2010,\n",
    "                           '2009':income_2009,\n",
    "                           '2008':income_2008,\n",
    "                         '2007':income_2007})\n",
    "\n",
    "\n",
    "\n",
    "#cleaning the income data\n",
    "for column in list(state_info.columns):\n",
    "    state_info[column]=state_info[column].map(lambda x : x.rstrip('\\n'))\n",
    "\n",
    "#turning the values into integers\n",
    "income_columns=list(state_info.columns)[1:]\n",
    "for column in income_columns:\n",
    "    state_info[column]=state_info[column].map(lambda x: int(x.replace('$','').replace(',','')))\n",
    "    \n",
    "\n",
    "#melted the dataframe so it can be merged into the original lending club dataframe\n",
    "state_info=state_info.melt(id_vars='state',\n",
    "               var_name='Date',\n",
    "               value_name='Income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The states in my main dataset are abbrevated so I needed to create a dictionary to convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T17:00:34.360221Z",
     "start_time": "2019-12-16T17:00:34.356446Z"
    }
   },
   "outputs": [],
   "source": [
    "#found a list online of states and their abbreviations that saved me a bit of time\n",
    "states_abr='''\n",
    "Alabama - AL\n",
    "Alaska - AK\n",
    "Arizona - AZ\n",
    "Arkansas - AR\n",
    "California - CA\n",
    "Colorado - CO\n",
    "Connecticut - CT\n",
    "Delaware - DE\n",
    "Florida - FL\n",
    "Georgia - GA\n",
    "Hawaii - HI\n",
    "Idaho - ID\n",
    "Illinois - IL\n",
    "Indiana - IN\n",
    "Iowa - IA\n",
    "Kansas - KS\n",
    "Kentucky - KY\n",
    "Louisiana - LA\n",
    "Maine - ME\n",
    "Maryland - MD\n",
    "Massachusetts - MA\n",
    "Michigan - MI\n",
    "Minnesota - MN\n",
    "Mississippi - MS\n",
    "Missouri - MO\n",
    "Montana - MT\n",
    "Nebraska - NE\n",
    "Nevada - NV\n",
    "New Hampshire - NH\n",
    "New Jersey - NJ\n",
    "New Mexico - NM\n",
    "New York - NY\n",
    "North Carolina - NC\n",
    "North Dakota - ND\n",
    "Ohio - OH\n",
    "Oklahoma - OK\n",
    "Oregon - OR\n",
    "Pennsylvania - PA\n",
    "Puerto Rico - PR\n",
    "Rhode Island - RI\n",
    "South Carolina - SC\n",
    "South Dakota - SD\n",
    "Tennessee - TN\n",
    "Texas - TX\n",
    "Utah - UT\n",
    "Vermont - VT\n",
    "Virginia - VA\n",
    "Washington - WA\n",
    "West Virginia - WV\n",
    "Wisconsin - WI\n",
    "Wyoming - WY'''\n",
    "\n",
    "#turning the string into a list\n",
    "states_abr_list=states_abr.split('\\n')\n",
    "\n",
    "#turning the list into a dict\n",
    "sl=states_abr_list[1:]\n",
    "states_tup_list=[(x[0],x[1]) for x in [x.replace(' ','').split('-') for x in sl]]\n",
    "state_dict=dict(states_tup_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T17:01:44.088542Z",
     "start_time": "2019-12-16T17:01:44.084603Z"
    }
   },
   "outputs": [],
   "source": [
    "#function to apply the transformation to the dataset (DC was not in the original list)\n",
    "def state_to_abr(cell):\n",
    "    x=cell.replace(' ','')\n",
    "    try:\n",
    "        return state_dict[x]\n",
    "    except:\n",
    "        \n",
    "        \n",
    "        return 'DC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T17:01:52.429545Z",
     "start_time": "2019-12-16T17:01:52.109217Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d5aad7589c26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abreviation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_to_abr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'state_info' is not defined"
     ]
    }
   ],
   "source": [
    "state_info['abreviation']=state_info.state.map(state_to_abr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the columns \n",
    "state_info.columns=['State_full','Year','average_income','abreviation']\n",
    "\n",
    "#saving the dataset to csv just in case it gets removed from the internet\n",
    "state_info.to_csv('state_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the two dataframes\n",
    "merged_df=pd.merge(df,state_info,left_on=['Year','addr_state'],right_on=['Year','abreviation'])\n",
    "#once merged did not need the state names again so we can drop them\n",
    "merged_df.drop(columns=['abreviation','State_full'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the dataframes merged correctly\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Percentile Income Bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuation of the above thought process but on a slightly more sophisticated level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['annual_inc_joint']=merged_df.apply(lambda x: x['annual_inc'] if np.isnan(x['annual_inc_joint']) else x['annual_inc'],axis=1)\n",
    "\n",
    "#removing the couple missing salaries\n",
    "merged_df=merged_df[merged_df.annual_inc_joint.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_state_incomes=pd.read_csv('states_median_income.csv',index_col=0,header=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformatting the columns here\n",
    "estimate_columns=[]\n",
    "for column in messy_state_incomes.columns:\n",
    "    try:\n",
    "        int(column.split('.')[1])\n",
    "    except:\n",
    "        estimate_columns.append(column)\n",
    "non_estimate_columns=[col for col in messy_state_incomes.columns if col not in estimate_columns]\n",
    "\n",
    "messy_state_incomes.drop(columns=non_estimate_columns,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_state_incomes.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting out the income bands\n",
    "percentile_state_incomes=messy_state_incomes.iloc[1:-8,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more reformatting to get the values into integers\n",
    "percentile_state_incomes= percentile_state_incomes.applymap(lambda x: x.replace(',',''))\n",
    "percentile_state_incomes=percentile_state_incomes.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the salary information was not in the form of percentiles we had to convert it to percentiles using cumsum. Below is one example, and then I do it to a whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(percentile_state_incomes['Alabama']/percentile_state_incomes['Alabama'].sum()).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_state_incomes\n",
    "for column in percentile_state_incomes.columns:    \n",
    "    percentile_state_incomes['percentile_{}'.format(column)]=(percentile_state_incomes[column]/percentile_state_incomes[column].sum()).cumsum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up the indexes of the dataframe to reflect the upper percentile bands and remove the unneeded punctuation\n",
    "\n",
    "clean_list=[]\n",
    "for ind in list(percentile_state_incomes.index):\n",
    "    try:\n",
    "        clean_list.append(int(ind.split('$')[-1].replace(',','')))\n",
    "    except:\n",
    "        clean_list.append(1000000)\n",
    "\n",
    "percentile_state_incomes['upper_income_bands']=clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now removing all the columns that are not percentile columns\n",
    "percentile_state_incomes=percentile_state_incomes.iloc[:,-53:]\n",
    "percentile_state_incomes.reset_index(inplace=True)\n",
    "percentile_state_incomes.drop(columns='index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melting the dataframe so we can merge it with our main dataframe on two columns- state and income band\n",
    "melted_percentiles=percentile_state_incomes.melt(id_vars='upper_income_bands',\n",
    "                             var_name='state',\n",
    "                             value_name='percentile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facing the same problem as above with states not being abbreviated\n",
    "melted_percentiles.state= melted_percentiles.state.map(lambda x: state_to_abr(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_percentile_for_income(x_row):\n",
    "    #this is only for 2014 data so going to put a if statement at the top, in the future I would like to expand to other years\n",
    "    if x_row['Year']=='2014':\n",
    "        salary=x_row['annual_inc']\n",
    "        salary_band=0\n",
    "        #think splitting in the middle will mean the function will have to run less if statements\n",
    "        if salary > 49999:\n",
    "            if salary < 74999:\n",
    "                salary_band=74999\n",
    "            elif salary < 99999:\n",
    "                salary_band=99999\n",
    "            elif salary < 149999:\n",
    "                salary_band= 149999\n",
    "            elif salary < 199999:\n",
    "                salary_band= 199999\n",
    "            else:\n",
    "                salary_band=1000000\n",
    "        else:\n",
    "            if salary >34999:\n",
    "                salary_band=49999\n",
    "            if salary > 24999:\n",
    "                salary_band=34999\n",
    "            if salary > 14999:\n",
    "                salary_band=24999\n",
    "            if salary > 10000:\n",
    "                salary_band=14999\n",
    "            else:\n",
    "                salary_band=10000\n",
    "\n",
    "        state=x_row['addr_state']\n",
    "        return melted_percentiles[(melted_percentiles['upper_income_bands']==salary_band)&(melted_percentiles['state']==state)]['percentile'].values[0]\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#here i could have used pandas cut but had already made the function when i realised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying to the whole dataframe\n",
    "merged_df['income_percentile_for_state']=merged_df.apply(find_percentile_for_income,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if there are any values set to 0\n",
    "merged_df[merged_df.income_percentile_for_state==0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting list of missing values\n",
    "merged_df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "all_data_na=merged_df.isna().sum()/len(merged_df)*100\n",
    "all_data_na=all_data_na.drop(all_data_na[all_data_na==0].index).sort_values(ascending=False)\n",
    "all_data_na[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just going to group most of the cleaning together so the code is a bit cleaner\n",
    "def cleaner(data,min_list=None,max_list=None,cat_list=None,date_list=None):\n",
    "    \n",
    "    \n",
    "    all_data_na=data.isna().sum()/len(data)*100\n",
    "    all_data_na=all_data_na.drop(all_data_na[all_data_na==0].index).sort_values(ascending=False)\n",
    "    print('columns with missing data before clean-\\n',all_data_na)\n",
    "    print('-'*20)\n",
    "    \n",
    "    if cat_list != None:\n",
    "        \n",
    "        for column in cat_list:\n",
    "            data[column].fillna(' ', inplace=True)\n",
    "    \n",
    "    if date_list != None:\n",
    "       \n",
    "        for column in date_list:\n",
    "            data[column]=pd.to_datetime(data[column],infer_datetime_format=True)\n",
    "            \n",
    "            \n",
    "    if min_list!=None:\n",
    "        \n",
    "        for column in min_list:\n",
    "            data[column].fillna((data[column].min()),inplace=True)\n",
    "    \n",
    "    if max_list!=None:\n",
    "        \n",
    "        for column in max_list:\n",
    "            data[column].fillna((data[column].max()),inplace=True)\n",
    "    \n",
    "    #then drop dregs\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    all_data_na=data.isna().sum()/len(data)*100\n",
    "    all_data_na=all_data_na.drop(all_data_na[all_data_na==0].index).sort_values(ascending=False)\n",
    "    print('columns with missing data after clean-\\n',all_data_na)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#did this because if only one person the joint dti is basically the normal dti\n",
    "merged_df['dti_joint']=merged_df.apply(lambda x: x['dti'] if np.isnan(x['dti_joint']) else x['dti_joint'],axis=1)\n",
    "#no info on this\n",
    "merged_df.drop(columns='desc',inplace=True)\n",
    "merged_df=merged_df[np.isfinite(merged_df['avg_cur_bal'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for categorical list because they were strings could leave them blank, assumed that for things like emp_title if they were blank it meant they were unemployed\n",
    "cat_list=['verification_status_joint','next_pymnt_d',\n",
    "          'emp_title','emp_length','title']\n",
    "\n",
    "#for most of these I thought that if they were blank it meant that it was because they did not have/do whatever the column was asking for eg inq last 12 months being blank- no inquiries in last 12 months\n",
    "min_list=['inq_last_12m','open_il_6m','inq_fi','max_bal_bc',\n",
    "         'open_rv_24m','open_rv_12m','total_bal_il','open_il_12m','open_il_24m',\n",
    "          'total_cu_tl','open_acc_6m','il_util','num_tl_120dpd_2m',\n",
    "          'num_tl_120dpd_2m','bc_util','percent_bc_gt_75','bc_open_to_buy','mths_since_recent_bc','last_pymnt_d',\n",
    "          'revol_util','pct_tl_nvr_dlq','last_credit_pull_d','all_util','mo_sin_old_il_acct'\n",
    "         ]\n",
    "\n",
    "#for max list thought that for these columns the person has not done whatever the column is looking for eg go delinqent on a loan\n",
    "max_list=['mths_since_last_record','mths_since_recent_bc_dlq','mths_since_last_major_derog',\n",
    "         'mths_since_recent_revol_delinq','mths_since_last_delinq','mths_since_recent_inq','mths_since_rcnt_il']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=cleaner(merged_df,min_list=min_list,max_list=max_list,cat_list=cat_list,date_list=['last_credit_pull_d','last_pymnt_d','earliest_cr_line','issue_d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to see what the distribution of fully paid/ good loans vs charged off and defaulted loans are. The way I have removed Na values has balanced the dataset somewhat which is good for my models in the short term as we will not have to employ measures such as SMOTE or undersampling. However in the future I would like to go back and include more data and therefore probably imbalance the dataset further in the hope to make my models more robust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loan_status.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns={'average_income':'average_income_state'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to compare salary vs state average - can imagine money goes a lot further in some states vs others\n",
    "\n",
    "merged_df['income_vs_average']=merged_df['annual_inc']-merged_df['average_income_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id and member id give no valuable info\n",
    "#may drop next payment d too because only got 3 different values\n",
    "\n",
    "merged_df.drop(columns=['id','member_id','next_pymnt_d','collection_recovery_fee'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['earliest_cr_line']=pd.to_datetime(merged_df['earliest_cr_line'],infer_datetime_format=True)\n",
    "merged_df['issue_d']=pd.to_datetime(merged_df['issue_d'],infer_datetime_format=True)\n",
    "merged_df['time_since first_cr_line']=merged_df['issue_d']-merged_df['earliest_cr_line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_to_float(d):\n",
    "    return d.timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['issue_d']=merged_df['issue_d'].map(datetime_to_float)\n",
    "\n",
    "\n",
    "date_columns=['earliest_cr_line','last_pymnt_d','last_credit_pull_d']\n",
    "\n",
    "for column in date_columns:\n",
    "    merged_df[column]=merged_df[column].map(datetime_to_float)\n",
    "\n",
    "merged_df['time_since first_cr_line']=merged_df['time_since first_cr_line'].apply(lambda x: float(x.days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on what loan statuses should be good vs bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are deciding on our classes- what is a good loan and what is a bad loan\n",
    "#think we can say that good loans are- fully paid, in grace period and issued with the rest being bad loans\n",
    "good_loans=['Fully Paid','In Grace Period','Issued']\n",
    "\n",
    "merged_df['good_or_bad']=merged_df.loan_status.map(lambda x : 1 if x not in good_loans else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Dataset was still very large and therefore any model I tried to run took far too long, I decided to focus on 2014 loans that were of a grade C or below. This is because the loans with lower grades had higher interest rates but were more likely to fail- having higher upside but also higher risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df=merged_df.copy()\n",
    "\n",
    "temp_df=temp_df[~temp_df.Year.isin(['2007','2008','2009','2010','2012','2016','2013','2015'])]\n",
    "\n",
    "temp_df=temp_df[temp_df.annual_inc<1000000]\n",
    "#removing outliers\n",
    "\n",
    "temp_df=temp_df[~temp_df.grade.isin(['A','B'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv('temp_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I had some time left and I was not happy with the scores I was getting I decided to do some very simple NLP on the job titles column to see if there was any predictive power in this column. My thought process in the below code was to pick out words which appeared quite a lot in the dataset (such as manager, driver etc) and then out of that set only include them if they had any predictive power so as to reduce the complexity of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only want popular words/jobs so it will generalise well\n",
    "cvec = CountVectorizer(token_pattern='\\w+',min_df=10000,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a count of words that appear in bad loans\n",
    "bad_loans=temp_df[temp_df['good_or_bad']==1]\n",
    "badcvec= CountVectorizer(token_pattern='\\w+',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badwords=badcvec.fit_transform(bad_loans.emp_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badwords.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad=pd.DataFrame(badwords.sum(axis=0),columns=badcvec.get_feature_names()).transpose().sort_values(0, ascending=False).transpose("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a count of words that appear in good loans\n",
    "good_loans=temp_df[temp_df['good_or_bad']==0]\n",
    "goodcvec= CountVectorizer(token_pattern='\\w+',binary=True)\n",
    "\n",
    "goodwords=goodcvec.fit_transform(good_loans.emp_title)\n",
    "\n",
    "goodwords.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning the matrix into a dataframe for ease of use\n",
    "good=pd.DataFrame(goodwords.sum(axis=0),columns=goodcvec.get_feature_names()).transpose().sort_values(0, ascending=False).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning the column names into a column \n",
    "good=good.melt(var_name='good_job',value_name='good_values')\n",
    "bad=bad.melt(var_name='bad_job',value_name='bad_values')\n",
    "jobs_performance=good.merge(bad,left_on='good_job',right_on='bad_job')\n",
    "#finding the good loan vs bad loan \n",
    "jobs_performance['good_loan_ratio']=jobs_performance['good_values']/(jobs_performance['good_values']+jobs_performance['bad_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keeping words that appear more than 1000 times\n",
    "jobs_performance=jobs_performance[(jobs_performance['good_values']+jobs_performance['bad_values'])>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_performance.sort_values(by='good_loan_ratio',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a function that binarizes the words with a given threshold, there is a better way of doing this out there I think\n",
    "def discriminator_jobs(x):\n",
    "    ''' returns job if the good to bad ratio is either \n",
    "    a lot higher than base or a lot lower than base'''\n",
    "    \n",
    "    if x['good_loan_ratio']>0.72 or x['good_loan_ratio']<0.62:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the function to the dataframe\n",
    "jobs_performance['discriminating_job']=jobs_performance.apply(discriminator_jobs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminating_jobs=list(jobs_performance[jobs_performance.discriminating_job==1]['good_job'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(token_pattern='\\w+',vocabulary=discriminating_jobs,\n",
    "                       binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=cvec.fit_transform(temp_df.emp_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df=pd.DataFrame(words.toarray(),\n",
    "                      columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_for_words=temp_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_for_words=merged_for_words.join(words_df)\n",
    "merged_for_words.drop(columns=['emp_title','total_rec_prncp',\n",
    "                      'out_prncp_inv','total_pymnt_inv','funded_amnt_inv',\n",
    "                      'last_pymnt_d','out_prncp','total_pymnt','total_rec_int','last_pymnt_amnt','index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_for_words.to_csv('merged_for_words1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
